# ChatGPT와 같은 대형 언어 모델(LLM)을 개발할 때 사용된 핵심 기술

---

### 🔁 **1단계: 언어 모델의 기본 구조 개발 (딥러닝 + Transformer)**

* **기술**: Transformer (2017년 등장, 논문 "Attention is All You Need")
* **역할**: 문장의 문맥을 이해하고, 다음 단어를 예측하는 데 사용됨
* **과정**:

  * 대규모 텍스트 데이터로 **지도학습(Supervised Learning)** 수행
  * 모델은 "다음 단어 맞히기(next token prediction)" 과제를 학습
* **예시**: GPT-1, GPT-2

---

### ⚙️ **2단계: 모델의 확장과 성능 향상 (대규모 사전학습 + 딥러닝 기술 최적화)**

* **기술**: 대규모 사전학습(Pretraining), Position Encoding, Multi-head Attention 등
* **역할**:

  * 더 많은 데이터, 더 큰 모델, 더 긴 문맥을 학습
  * 문법, 지식, 추론 능력이 자연스럽게 향상
* **예시**: GPT-3 (175B 파라미터, 2020년)

---

### 🧠 **3단계: 강화학습 기반의 미세조정 (RLHF: Reinforcement Learning with Human Feedback)**

* **기술**:

  * **Human Preference Data 수집** → 사람들이 선호하는 응답 쌍을 라벨링
  * **보상 모델(Reward Model)** 학습
  * **PPO (Proximal Policy Optimization)** 를 사용해 모델을 강화학습으로 미세조정
* **역할**:

  * 모델의 출력을 사람이 더 좋아하는 방향으로 조정
  * 거칠고 비윤리적인 응답을 줄이고, 유용한 응답을 늘림
* **예시**: ChatGPT (GPT-3.5), GPT-4

---

### 📚 **4단계: 안전성과 유용성 향상을 위한 추가 기술**

* **기술들**:

  * **헬루시네이션 감소**: RAG (Retrieval-Augmented Generation), 지식 기반 응답
  * **시스템 메시지 및 역할 기반 응답 제어**
  * **Tool Use**: 코드 실행, 검색, 이미지 생성 등 외부 도구 연동
  * **멀티모달 학습** (텍스트 + 이미지 등)
* **역할**:

  * 실제 사용 환경에서 더 유용하고 신뢰할 수 있도록 모델 튜닝
  * 사람과 상호작용할 수 있는 에이전트로 발전

---

### 🧩 기술 순서 요약표

| 순서 | 주요 기술                   | 핵심 역할            | 대표 모델         |
| -- | ----------------------- | ---------------- | ------------- |
| 1  | Transformer             | 언어 이해와 생성의 기반 구조 | GPT-1, 2      |
| 2  | 대규모 딥러닝 사전학습            | 방대한 지식과 문맥 학습    | GPT-3         |
| 3  | RLHF (강화학습 + 인간 피드백)    | 인간이 선호하는 응답 학습   | ChatGPT       |
| 4  | 도구 사용, 검색 기반 생성, 멀티모달 등 | 안전하고 유용한 AI로 진화  | GPT-4, GPT-4o |
